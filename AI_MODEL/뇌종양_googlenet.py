# -*- coding: utf-8 -*-
"""뇌종양_googlenet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/173ftyRFM6ys5sqVSYK6kQQJ7w1QYNuHy
"""

#다운받은 데이터셋 압축 풀기
!unzip brain.zip -d ./brain

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras import optimizers, Input
import glob, os
import numpy as np
import matplotlib.pyplot as plt

#Train / Test 폴더 생성
os.makedirs('./brain/train')
os.makedirs('./brain/test')

path_train = './brain/train'
path_test = './brain/test'

os.makedirs(path_train + '/nomal')
os.makedirs(path_test + '/nomal')

os.makedirs(path_train + '/glioma')
os.makedirs(path_test + '/glioma')

os.makedirs(path_train + '/meningioma')
os.makedirs(path_test + '/meningioma')

os.makedirs(path_train + '/pituitary')
os.makedirs(path_test + '/pituitary')

path = './brain/Data'

Nomal = sorted(glob.glob(path + '/Normal' + '/*'))
glioma = sorted(glob.glob(path + '/Tumor/glioma_tumor' + '/*'))
meningioma = sorted(glob.glob(path + '/Tumor/meningioma_tumor' + '/*'))
pituitary = sorted(glob.glob(path + '/Tumor/pituitary_tumor' + '/*'))

print(' nomal 이미지 수 : {} \nglioma 이미지 수 : {} \nmeningioma 이미지 수 : {} \npituitary 이미지 수 : {}'.format(len(Nomal),
                                                                            len(glioma),
                                                                            len(meningioma),
                                                                            len(pituitary)))

import math

# 원본 이미지 파일 중 20%의 이미지 파일을 테스트 파일로 변경
Nomal_test = round(len(Nomal) * 0.2)
glioma_test = round(len(glioma) * 0.2)
meningioma_test = round(len(meningioma) * 0.2)
pituitary_test = round(len(pituitary) * 0.2)

print('nomal test : {}/{}'.format(Nomal_test, len(Nomal)))
print('glioma test : {}/{}'.format(glioma_test, len(glioma)))
print('meningioma test : {}/{}'.format(meningioma_test, len(meningioma)))
print('pituitary test : {}/{}'.format(pituitary_test, len(pituitary)))

#train / test 파일 경로

#train_path = '/brain/train'
#test_path = './brain/test'

Nomal_train_path = './brain/train/nomal'
Nomal_test_path = './brain/test/nomal'

glioma_train_path = './brain/train/glioma'
glioma_test_path = './brain/test/glioma'

meningioma_train_path = './brain/train/meningioma'
meningioma_test_path = './brain/test/meningioma'

pituitary_train_path = './brain/train/pituitary'
pituitary_test_path = './brain/test/pituitary'

#train / test 폴더에 이미지 데이터 넣는 함수
import shutil, random

def split(img_list, test_count, path_train, path_test):

    test_files = []
    for i in random.sample(img_list, test_count):
        test_files.append(i)

    #차집합으로 train/test 리스트 생성
    train_files = [x for x in img_list if x not in test_files]

    for k in train_files:
        shutil.copy(k, path_train)

    for c in test_files:
        shutil.copy(c, path_test)

    print('train 폴더 : {} \ntest 폴더 : {}'.format(len(glob.glob(path_train + '/*')), len(glob.glob(path_test + '/*'))))

split(Nomal, Nomal_test, Nomal_train_path, Nomal_test_path)
split(glioma, glioma_test, glioma_train_path, glioma_test_path)
split(meningioma, meningioma_test, meningioma_train_path, meningioma_test_path)
split(pituitary, pituitary_test, pituitary_train_path, pituitary_test_path)

#폴더 삭제

#shutil.rmtree('./brain/test')
#shutil.rmtree('./brain/train')
#shutil.rmtree('./brain/test/nomal/')
#shutil.rmtree('./brain/test/glioma/')
#shutil.rmtree('./brain/test/meningioma/')
#shutil.rmtree('./brain/test/pituitary/')
#shutil.rmtree('./brain/train/nomal/')
#shutil.rmtree('./brain/train/glioma/')
#shutil.rmtree('./brain/train/meningioma/')
#shutil.rmtree('./brain/train/pituitary/')

print('train 폴더 : {} \ntest 폴더 : {}'.format(len(glob.glob(path_train + '/*')), len(glob.glob(path_test + '/*'))))

train_datagen = ImageDataGenerator(#rescale = 1. / 255,
                                   width_shift_range = 0.2,
                                   height_shift_range = 0.2,
                                   shear_range = 0.15,
                                   #rotation_range = 5,
                                   #zoom)range = [0.9, 2.2],
                                   #verticla_flip = True,
                                   #fill_mode = 'mearest'
)

train_generator = train_datagen.flow_from_directory(
    './brain/train/',
    target_size = (224, 224),
    batch_size = 64,
    shuffle=True,
    class_mode = 'categorical')

test_datagen = ImageDataGenerator()#rescale = 1. / 255)

test_generator = test_datagen.flow_from_directory(
    './brain/test/',
    target_size = (224, 224),
    batch_size = 64,
    shuffle=True,
    class_mode = 'categorical')

X_train, y_train = train_generator.next()

X_test, y_test = test_generator.next()

print("Y_train:", y_train.shape)

from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Input, Dense, Dropout, AveragePooling2D, concatenate, Flatten
from keras.preprocessing.image import ImageDataGenerator
from keras.models import Model
from keras import models

def Inception_block(input_layer, i1, i2_conv1, i2_conv3, i3_conv1, i3_conv5, i4):
  # Input:
  # - f1: number of filters of the 1x1 convolutional layer in the first path
  # - f2_conv1, f2_conv3 are number of filters corresponding to the 1x1 and 3x3 convolutional layers in the second path
  # - f3_conv1, f3_conv5 are the number of filters corresponding to the 1x1 and 5x5  convolutional layer in the third path
  # - f4: number of filters of the 1x1 convolutional layer in the fourth path

  # 1st path:
  path1 = Conv2D(filters=i1, kernel_size = (1,1), padding = 'same', activation = 'relu')(input_layer)

  # 2nd path
  path2 = Conv2D(filters = i2_conv1, kernel_size = (1,1), padding = 'same', activation = 'relu')(input_layer)
  path2 = Conv2D(filters = i2_conv3, kernel_size = (3,3), padding = 'same', activation = 'relu')(path2)

  # 3rd path
  path3 = Conv2D(filters = i3_conv1, kernel_size = (1,1), padding = 'same', activation = 'relu')(input_layer)
  path3 = Conv2D(filters = i3_conv5, kernel_size = (5,5), padding = 'same', activation = 'relu')(path3)

  # 4th path
  path4 = MaxPooling2D((3,3), strides= (1,1), padding = 'same')(input_layer)
  path4 = Conv2D(filters = i4, kernel_size = (1,1), padding = 'same', activation = 'relu')(path4)

  output_layer = concatenate([path1, path2, path3, path4], axis = -1)

  return output_layer

# input layer
input_layer = Input(shape = (224, 224, 3))

# convolutional layer: filters = 64, kernel_size = (7,7), strides = 2
X = Conv2D(filters = 64, kernel_size = (7,7), strides = 2, padding = 'same', activation = 'relu')(input_layer)

# max-pooling layer: pool_size = (3,3), strides = 2
X = MaxPooling2D(pool_size = (3,3), padding = 'same', strides = 2)(X)

# convolutional layer: filters = 64, strides = 1
X = Conv2D(filters = 64, kernel_size = (1,1), strides = 1, padding = 'valid', activation = 'relu')(X)

# convolutional layer: filters = 192, kernel_size = (3,3)
X = Conv2D(filters = 192, kernel_size = (3,3), padding = 'same', activation = 'relu')(X)

# max-pooling layer: pool_size = (3,3), strides = 2
X = MaxPooling2D(pool_size= (3,3), padding = 'same', strides = 2)(X)

# 1st Inception block
X = Inception_block(X, i1 = 64, i2_conv1 = 96, i2_conv3 = 128, i3_conv1 = 16, i3_conv5 = 32, i4 = 32)

# 2nd Inception block
X = Inception_block(X, i1 = 128, i2_conv1 = 128, i2_conv3 = 192, i3_conv1 = 32, i3_conv5 = 96, i4 = 64)

# max-pooling layer: pool_size = (3,3), strides = 2
X = MaxPooling2D(pool_size= (3,3), padding = 'same', strides = 2)(X)

# 3rd Inception block
X = Inception_block(X, i1 = 192, i2_conv1 = 96, i2_conv3 = 208, i3_conv1 = 16, i3_conv5 = 48, i4 = 64)

# Extra network 1:
X1 = AveragePooling2D(pool_size = (5,5), padding = 'valid', strides = 3)(X)
X1 = Conv2D(filters = 128, kernel_size = (1,1), padding = 'same', activation = 'relu')(X1)
X1 = Flatten()(X1)
X1 = Dense(1024, activation = 'relu')(X1)
X1 = Dense(1024, activation = 'relu')(X1)
X1 = Dropout(0.7)(X1)
X1 = Dense(4, activation = 'softmax')(X1)


# 4th Inception block
X = Inception_block(X, i1 = 160, i2_conv1 = 112, i2_conv3 = 224, i3_conv1 = 24, i3_conv5 = 64, i4 = 64)

# 5th Inception block
X = Inception_block(X, i1 = 128, i2_conv1 = 128, i2_conv3 = 256, i3_conv1 = 24, i3_conv5 = 64, i4 = 64)

# 6th Inception block
X = Inception_block(X, i1 = 112, i2_conv1 = 144, i2_conv3 = 288, i3_conv1 = 32, i3_conv5 = 64, i4 = 64)

# Extra network 2:
X2 = AveragePooling2D(pool_size = (5,5), padding = 'valid', strides = 3)(X)
X2 = Conv2D(filters = 128, kernel_size = (1,1), padding = 'same', activation = 'relu')(X2)
X2 = Flatten()(X2)
X2 = Dense(1024, activation = 'relu')(X2)
X2 = Dense(1024, activation = 'relu')(X2)
X2 = Dropout(0.7)(X2)
X2 = Dense(4, activation = 'softmax')(X2)


# 7th Inception block
X = Inception_block(X, i1 = 256, i2_conv1 = 160, i2_conv3 = 320, i3_conv1 = 32, i3_conv5 = 128, i4 = 128)

# max-pooling layer: pool_size = (3,3), strides = 2
X = MaxPooling2D(pool_size = (3,3), padding = 'same', strides = 2)(X)

# 8th Inception block
X = Inception_block(X, i1 = 256, i2_conv1 = 160, i2_conv3 = 320, i3_conv1 = 32, i3_conv5 = 128, i4 = 128)

# 9th Inception block
X = Inception_block(X, i1 = 384, i2_conv1 = 192, i2_conv3 = 384, i3_conv1 = 48, i3_conv5 = 128, i4 = 128)

# max-pooling layer: pool_size = (7,7), strides = 1
X = AveragePooling2D(pool_size=(7,7), strides = (1,1), padding = 'valid')(X)
X = Dropout(0.4)(X)
X = Flatten()(X)

# output layer
X = Dense(1000, activation = 'relu')(X)
X = Dense(4, activation = 'softmax')(X)

# model
model = Model(input_layer, [X, X1, X2], name = 'GoogLeNet')

model.summary()

print(input_layer.shape)
print(X.shape)
print(X1.shape)
print(X2.shape)

model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])
early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 5)

history = model.fit(
    train_generator,
    epochs = 50,
    validation_data = test_generator,
    validation_steps = 10,
    callbacks = [early_stopping_callback])

y_vloss = history.history['val_loss']
y_loss = history.history['loss']

x_len = np.arange(len(y_loss))
plt.plot(x_len, y_vloss, marker = ',' , c = 'red', label = 'Testset_loss')
plt.plot(x_len, y_loss, marker = ',' , c = 'blue', label = 'Trainset_loss')

plt.legend(loc = 'upper right')
plt.grid()
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()

plt.plot(history.history['dense_13_accuracy'])
plt.plot(history.history['val_dense_13_accuracy'])
plt.title('Accuracy')
plt.legend(['train','test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Loss')
plt.legend(['train','test'], loc='upper left')
plt.show()